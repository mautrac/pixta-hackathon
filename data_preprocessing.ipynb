{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":7338576,"sourceType":"datasetVersion","datasetId":4260595},{"sourceId":7371344,"sourceType":"datasetVersion","datasetId":4282934},{"sourceId":7394078,"sourceType":"datasetVersion","datasetId":4298684}],"dockerImageVersionId":30627,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"data_path = \"/kaggle/input/fac-data-p1/data/mnt/md0/projects/sami-hackathon/private/data/\"\nlabel_path = \"/kaggle/input/fac-data-p1/labels.csv\"","metadata":{"execution":{"iopub.status.busy":"2024-01-07T14:49:56.074187Z","iopub.execute_input":"2024-01-07T14:49:56.075364Z","iopub.status.idle":"2024-01-07T14:49:56.079761Z","shell.execute_reply.started":"2024-01-07T14:49:56.075327Z","shell.execute_reply":"2024-01-07T14:49:56.078696Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip3 install \"git+https://github.com/philferriere/cocoapi.git#egg=pycocotools&subdirectory=PythonAPI\"\n!pip3 install cython\n!pip3 install -U pip && pip3 install -r /kaggle/input/yolox-test/yolox_face/requirements.txt","metadata":{"execution":{"iopub.status.busy":"2024-01-09T16:54:13.375471Z","iopub.execute_input":"2024-01-09T16:54:13.375961Z","iopub.status.idle":"2024-01-09T16:55:39.367105Z","shell.execute_reply.started":"2024-01-09T16:54:13.37593Z","shell.execute_reply":"2024-01-09T16:55:39.366018Z"},"scrolled":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nimport time\nfrom loguru import logger\n\nimport cv2\nimport sys\nsys.path.insert(1, '/kaggle/input/yolox-test/yolox_face/')\nimport torch\nfrom yolox.data.data_augment import ValTransform\nfrom yolox.data.datasets.voc_classes import VOC_CLASSES\nfrom yolox.exp import get_exp\nfrom yolox.utils import get_model_info, postprocess, vis","metadata":{"execution":{"iopub.status.busy":"2024-01-13T13:44:21.326321Z","iopub.execute_input":"2024-01-13T13:44:21.3266Z","iopub.status.idle":"2024-01-13T13:44:21.366873Z","shell.execute_reply.started":"2024-01-13T13:44:21.326575Z","shell.execute_reply":"2024-01-13T13:44:21.365303Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Predictor(object):\n    def __init__(\n        self,\n        model,\n        exp,\n        cls_names=VOC_CLASSES,\n        device=\"gpu\",\n    ):\n        self.model = model\n        self.cls_names = cls_names\n        self.num_classes = exp.num_classes\n        self.confthre = exp.test_conf\n        self.nmsthre = exp.nmsthre\n        self.test_size = exp.test_size\n        self.preproc = ValTransform(legacy=False)\n        self.device = device\n\n    def inference(self, img):\n        img_info = {\"id\": 0}\n        if isinstance(img, str):\n            img_info[\"file_name\"] = os.path.basename(img)\n            img = cv2.imread(img)\n        else:\n            img_info[\"file_name\"] = None\n\n        height, width = img.shape[:2]\n        img_info[\"height\"] = height\n        img_info[\"width\"] = width\n        img_info[\"raw_img\"] = img\n\n        ratio = min(self.test_size[0] / img.shape[0], self.test_size[1] / img.shape[1])\n        img_info[\"ratio\"] = ratio\n\n        img, _ = self.preproc(img, None, self.test_size)\n        img = torch.from_numpy(img).unsqueeze(0)\n        img = img.float()\n        if self.device == \"gpu\":\n            img = img.cuda()\n\n        with torch.no_grad():\n            t0 = time.time()\n            outputs = self.model(img)\n            outputs = postprocess(\n                outputs, self.num_classes, self.confthre,\n                self.nmsthre, class_agnostic=True\n            )\n            logger.info(\"Infer time: {:.4f}s\".format(time.time() - t0))\n        return outputs, img_info\n\n    def visual(self, output, img_info, cls_conf=0.35):\n        ratio = img_info[\"ratio\"]\n        img = img_info[\"raw_img\"]\n        if output is None:\n            return img\n        output = output.cpu()\n\n        bboxes = output[:, 0:4]\n\n        # preprocessing: resize\n        bboxes /= ratio\n\n        cls = output[:, 6]\n        scores = output[:, 4] * output[:, 5]\n        #x0, y0, x1, y1\n        vis_res = vis(img, bboxes, scores, cls, cls_conf, self.cls_names)\n        return bboxes, vis_res\n\n\ndef image_demo(predictor, vis_folder, path, current_time):\n    files = [path]\n    files.sort()\n    bb = []\n    for image_name in files:\n        outputs, img_info = predictor.inference(image_name)\n        bboxes, result_image = predictor.visual(outputs[0], img_info, predictor.confthre)\n        save_folder = os.path.join(\n            vis_folder, time.strftime(\"%Y_%m_%d_%H_%M_%S\", current_time)\n        )\n        os.makedirs(save_folder, exist_ok=True)\n        save_file_name = os.path.join(save_folder, os.path.basename(image_name))\n        logger.info(\"Saving detection result in {}\".format(save_file_name))\n        cv2.imwrite(save_file_name, result_image)\n        logger.info(\"Detect face at {}\".format(bboxes))\n        bb.append(bboxes)\n    return bb","metadata":{"execution":{"iopub.status.busy":"2024-01-13T13:44:21.36774Z","iopub.status.idle":"2024-01-13T13:44:21.368109Z","shell.execute_reply.started":"2024-01-13T13:44:21.367926Z","shell.execute_reply":"2024-01-13T13:44:21.367942Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"exp = get_exp('/kaggle/input/yolox-test/yolox_face/yolox_voc_s.py', None)\nexp.test_conf = 0.25\nexp.nmsthre = 0.45\nexp.test_size = (640, 640)\nmodel = exp.get_model()\nlogger.info(\"Model Summary: {}\".format(get_model_info(model, exp.test_size)))\nmodel.cuda()\nmodel.eval()\nckpt_file = '/kaggle/input/yolox-test/yolox_face/checkpoint.pth'\nlogger.info(\"loading checkpoint\")\nckpt = torch.load(ckpt_file, map_location=\"cpu\")\nmodel.load_state_dict(ckpt[\"model\"])\nlogger.info(\"loaded checkpoint done.\")\ntrt_file = None\ndecoder = None\npredictor = Predictor(\n    model=model, exp=exp, cls_names=VOC_CLASSES, device=\"gpu\"\n)\ncurrent_time = time.localtime()\n\nfile_name = os.path.join(exp.output_dir, exp.exp_name)\nvis_folder = os.path.join(file_name, \"vis_res\")\nbb = image_demo(\n    predictor=predictor,\n    vis_folder='result',\n    path='/kaggle/input/fac-data-p1/data/mnt/md0/projects/sami-hackathon/private/data/100013282.jpg',\n    current_time=current_time\n)\n","metadata":{"execution":{"iopub.status.busy":"2024-01-09T17:05:48.78169Z","iopub.execute_input":"2024-01-09T17:05:48.782057Z","iopub.status.idle":"2024-01-09T17:05:49.316448Z","shell.execute_reply.started":"2024-01-09T17:05:48.78203Z","shell.execute_reply":"2024-01-09T17:05:49.315456Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimg_test = cv2.imread('/kaggle/working/result/2024_01_09_16_59_02/100013282.jpg')\nplt.imshow(img_test)","metadata":{"execution":{"iopub.status.busy":"2024-01-09T17:05:51.885342Z","iopub.execute_input":"2024-01-09T17:05:51.885737Z","iopub.status.idle":"2024-01-09T17:05:52.757175Z","shell.execute_reply.started":"2024-01-09T17:05:51.885706Z","shell.execute_reply":"2024-01-09T17:05:52.755408Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"img_src = cv2.imread('/kaggle/input/fac-data-p1/data/mnt/md0/projects/sami-hackathon/private/data/100013282.jpg')\n#x0 y0 x1 y1\nbbox = bb[0].numpy().squeeze().astype(int)\nplt.imshow(img_src[bbox[1]:bbox[3], bbox[0]:bbox[2]])","metadata":{"execution":{"iopub.status.busy":"2024-01-09T17:07:38.243251Z","iopub.execute_input":"2024-01-09T17:07:38.244202Z","iopub.status.idle":"2024-01-09T17:07:38.662447Z","shell.execute_reply.started":"2024-01-09T17:07:38.244172Z","shell.execute_reply":"2024-01-09T17:07:38.661404Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%env CUDA_VISIBLE_DEVICES=1","metadata":{"execution":{"iopub.status.busy":"2024-01-13T09:03:27.026244Z","iopub.execute_input":"2024-01-13T09:03:27.026879Z","iopub.status.idle":"2024-01-13T09:03:27.032522Z","shell.execute_reply.started":"2024-01-13T09:03:27.026849Z","shell.execute_reply":"2024-01-13T09:03:27.031456Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nfrom PIL import Image\nimport cv2\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import preprocessing\nfrom sklearn import svm\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn import svm,metrics,preprocessing\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.decomposition import PCA\nfrom sklearn.pipeline import Pipeline\n\nfrom collections import defaultdict\nimport os\nimport random\nimport numpy as np\nfrom tqdm import tqdm\nimport time\nimport pickle","metadata":{"execution":{"iopub.status.busy":"2024-01-13T13:44:51.34983Z","iopub.execute_input":"2024-01-13T13:44:51.350216Z","iopub.status.idle":"2024-01-13T13:44:51.356921Z","shell.execute_reply.started":"2024-01-13T13:44:51.350183Z","shell.execute_reply":"2024-01-13T13:44:51.355844Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import tensorflow\nimport pathlib\nimport tensorflow as tf\nfrom tensorflow.keras.applications.resnet import preprocess_input\nfrom tensorflow.keras.layers import TimeDistributed, GRU, Dense, Dropout, Flatten, LSTM, Activation, MaxPooling2D\nfrom tensorflow.keras.regularizers import l2 as L2_reg\nfrom tensorflow.keras.layers import Conv2D, BatchNormalization, \\\n    MaxPool2D, GlobalMaxPool2D, Input, Masking, Conv3D, MaxPooling3D, GlobalMaxPool3D\nfrom tensorflow.keras.optimizers import SGD, Adam\n\nprint(tf.__version__)","metadata":{"execution":{"iopub.status.busy":"2024-01-13T13:44:54.314012Z","iopub.execute_input":"2024-01-13T13:44:54.314762Z","iopub.status.idle":"2024-01-13T13:44:54.433193Z","shell.execute_reply.started":"2024-01-13T13:44:54.314728Z","shell.execute_reply":"2024-01-13T13:44:54.432321Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\ndf = pd.read_csv(\"/kaggle/input/fac-data-p1/labels.csv\")\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2024-01-13T13:44:58.194674Z","iopub.execute_input":"2024-01-13T13:44:58.195409Z","iopub.status.idle":"2024-01-13T13:44:58.362687Z","shell.execute_reply.started":"2024-01-13T13:44:58.195375Z","shell.execute_reply":"2024-01-13T13:44:58.361708Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"image = cv2.imread('/kaggle/input/fac-data-p1/data/mnt/md0/projects/sami-hackathon/private/data/' + df['file_name'][2])\nplt.imshow(image)","metadata":{"execution":{"iopub.status.busy":"2024-01-13T16:15:08.218413Z","iopub.execute_input":"2024-01-13T16:15:08.218702Z","iopub.status.idle":"2024-01-13T16:15:09.08807Z","shell.execute_reply.started":"2024-01-13T16:15:08.218678Z","shell.execute_reply":"2024-01-13T16:15:09.08706Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"image = cv2.imread('/kaggle/input/fac-data-p1/data/mnt/md0/projects/sami-hackathon/private/data/' + df['file_name'][2])\nbb = eval(df['bbox'][2])\ny1 = int(bb[1])\ny2 = int(bb[1] + bb[3])\nx1 = int(bb[0])\nx2 = int(bb[0] + bb[2])\nimage_test = image[y1:y2, x1:x2]\nplt.imshow(image_test)","metadata":{"execution":{"iopub.status.busy":"2024-01-13T14:33:37.446632Z","iopub.execute_input":"2024-01-13T14:33:37.447513Z","iopub.status.idle":"2024-01-13T14:33:37.789248Z","shell.execute_reply.started":"2024-01-13T14:33:37.447481Z","shell.execute_reply":"2024-01-13T14:33:37.788275Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#age, (race, skintone), masked, emotion, gender\nemotions = df['emotion'].unique()\nraces = df['race'].unique()\nskintones = df['skintone'].unique()\nmaskeds = df['masked'].unique()\ngenders = df['gender'].unique()\nemotions, races, skintones, maskeds, genders","metadata":{"execution":{"iopub.status.busy":"2024-01-13T09:10:59.756669Z","iopub.execute_input":"2024-01-13T09:10:59.757064Z","iopub.status.idle":"2024-01-13T09:10:59.772565Z","shell.execute_reply.started":"2024-01-13T09:10:59.757031Z","shell.execute_reply":"2024-01-13T09:10:59.771413Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function","metadata":{"execution":{"iopub.status.busy":"2024-01-13T13:55:15.559958Z","iopub.execute_input":"2024-01-13T13:55:15.560771Z","iopub.status.idle":"2024-01-13T13:55:15.564675Z","shell.execute_reply.started":"2024-01-13T13:55:15.560739Z","shell.execute_reply":"2024-01-13T13:55:15.563741Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport sys\nimport os\nimport argparse\nimport tensorflow as tf\nimport numpy as np\nimport cv2\nimport time\n\nimport subprocess, re \n\n\ndef is_specialfile(path,exts):\n    _, file_extension = os.path.splitext(path)\n    return file_extension.lower() in exts\n\nimg_extensions=['.jpg','.jpeg','.png']\ndef is_image(path):\n    return is_specialfile(path,img_extensions)\n\nvideo_extensions=['.mov','.avi']\ndef is_video(path):\n    return is_specialfile(path,video_extensions)\n    \nclass FacialImageProcessing:\n    # minsize: minimum of faces' size\n    def __init__(self, print_stat=False, minsize = 32):\n        self.print_stat=print_stat\n        self.minsize=minsize\n        \n#         models_path,_ = os.path.split(os.path.realpath(__file__))\n        model_files={os.path.join('/kaggle/input/facial/facical_analysis','mtcnn.pb'):''}\n\n        with tf.Graph().as_default() as full_graph:\n            for model_file in model_files:\n                tf.import_graph_def(FacialImageProcessing.load_graph_def(model_file), name=model_files[model_file])\n        self.sess=tf.compat.v1.Session(graph=full_graph)#,config=tf.ConfigProto(device_count={'CPU':1,'GPU':0}))\n        self.pnet, self.rnet, self.onet = FacialImageProcessing.load_mtcnn(self.sess,full_graph)     \n\n    def close(self):\n        self.sess.close()\n    \n    @staticmethod\n    def load_graph_def(frozen_graph_filename):\n        graph_def=None\n        with tf.io.gfile.GFile(frozen_graph_filename, 'rb') as f:\n            graph_def = tf.compat.v1.GraphDef()\n            graph_def.ParseFromString(f.read())\n        return graph_def\n    \n    @staticmethod\n    def load_graph(frozen_graph_filename, prefix=''):\n        graph_def = FacialImageProcessing.load_graph_def(frozen_graph_filename)\n        with tf.Graph().as_default() as graph:\n            tf.import_graph_def(graph_def, name=prefix)\n        return graph\n\n    @staticmethod\n    def load_mtcnn(sess,graph):\n        pnet_out_1=graph.get_tensor_by_name('pnet/conv4-2/BiasAdd:0')\n        pnet_out_2=graph.get_tensor_by_name('pnet/prob1:0')\n        pnet_in=graph.get_tensor_by_name('pnet/input:0')\n        \n        rnet_out_1=graph.get_tensor_by_name('rnet/conv5-2/conv5-2:0')\n        rnet_out_2=graph.get_tensor_by_name('rnet/prob1:0')\n        rnet_in=graph.get_tensor_by_name('rnet/input:0')\n        \n        onet_out_1=graph.get_tensor_by_name('onet/conv6-2/conv6-2:0')\n        onet_out_2=graph.get_tensor_by_name('onet/conv6-3/conv6-3:0')\n        onet_out_3=graph.get_tensor_by_name('onet/prob1:0')\n        onet_in=graph.get_tensor_by_name('onet/input:0')\n        \n        pnet_fun = lambda img : sess.run((pnet_out_1, pnet_out_2), feed_dict={pnet_in:img})\n        rnet_fun = lambda img : sess.run((rnet_out_1, rnet_out_2), feed_dict={rnet_in:img})\n        onet_fun = lambda img : sess.run((onet_out_1, onet_out_2, onet_out_3), feed_dict={onet_in:img})\n        return pnet_fun, rnet_fun, onet_fun\n        \n    @staticmethod\n    def bbreg(boundingbox,reg):\n        # calibrate bounding boxes\n        if reg.shape[1]==1:\n            reg = np.reshape(reg, (reg.shape[2], reg.shape[3]))\n\n        w = boundingbox[:,2]-boundingbox[:,0]+1\n        h = boundingbox[:,3]-boundingbox[:,1]+1\n        b1 = boundingbox[:,0]+reg[:,0]*w\n        b2 = boundingbox[:,1]+reg[:,1]*h\n        b3 = boundingbox[:,2]+reg[:,2]*w\n        b4 = boundingbox[:,3]+reg[:,3]*h\n        boundingbox[:,0:4] = np.transpose(np.vstack([b1, b2, b3, b4 ]))\n        return boundingbox\n     \n    @staticmethod\n    def generateBoundingBox(imap, reg, scale, t):\n        # use heatmap to generate bounding boxes\n        stride=2\n        cellsize=12\n\n        imap = np.transpose(imap)\n        dx1 = np.transpose(reg[:,:,0])\n        dy1 = np.transpose(reg[:,:,1])\n        dx2 = np.transpose(reg[:,:,2])\n        dy2 = np.transpose(reg[:,:,3])\n        y, x = np.where(imap >= t)\n        if y.shape[0]==1:\n            dx1 = np.flipud(dx1)\n            dy1 = np.flipud(dy1)\n            dx2 = np.flipud(dx2)\n            dy2 = np.flipud(dy2)\n        score = imap[(y,x)]\n        reg = np.transpose(np.vstack([ dx1[(y,x)], dy1[(y,x)], dx2[(y,x)], dy2[(y,x)] ]))\n        if reg.size==0:\n            reg = np.empty((0,3))\n        bb = np.transpose(np.vstack([y,x]))\n        q1 = np.fix((stride*bb+1)/scale)\n        q2 = np.fix((stride*bb+cellsize-1+1)/scale)\n        boundingbox = np.hstack([q1, q2, np.expand_dims(score,1), reg])\n        return boundingbox, reg\n     \n    # function pick = nms(boxes,threshold,type)\n    @staticmethod\n    def nms(boxes, threshold, method):\n        if boxes.size==0:\n            return np.empty((0,3))\n        x1 = boxes[:,0]\n        y1 = boxes[:,1]\n        x2 = boxes[:,2]\n        y2 = boxes[:,3]\n        s = boxes[:,4]\n        area = (x2-x1+1) * (y2-y1+1)\n        I = np.argsort(s)\n        pick = np.zeros_like(s, dtype=np.int16)\n        counter = 0\n        while I.size>0:\n            i = I[-1]\n            pick[counter] = i\n            counter += 1\n            idx = I[0:-1]\n            xx1 = np.maximum(x1[i], x1[idx])\n            yy1 = np.maximum(y1[i], y1[idx])\n            xx2 = np.minimum(x2[i], x2[idx])\n            yy2 = np.minimum(y2[i], y2[idx])\n            w = np.maximum(0.0, xx2-xx1+1)\n            h = np.maximum(0.0, yy2-yy1+1)\n            inter = w * h\n            if method == 'Min':\n                o = inter / np.minimum(area[i], area[idx])\n            else:\n                o = inter / (area[i] + area[idx] - inter)\n            I = I[np.where(o<=threshold)]\n        pick = pick[0:counter]\n        return pick\n\n    # function [dy edy dx edx y ey x ex tmpw tmph] = pad(total_boxes,w,h)\n    @staticmethod\n    def pad(total_boxes, w, h):\n        # compute the padding coordinates (pad the bounding boxes to square)\n        tmpw = (total_boxes[:,2]-total_boxes[:,0]+1).astype(np.int32)\n        tmph = (total_boxes[:,3]-total_boxes[:,1]+1).astype(np.int32)\n        numbox = total_boxes.shape[0]\n\n        dx = np.ones((numbox), dtype=np.int32)\n        dy = np.ones((numbox), dtype=np.int32)\n        edx = tmpw.copy().astype(np.int32)\n        edy = tmph.copy().astype(np.int32)\n\n        x = total_boxes[:,0].copy().astype(np.int32)\n        y = total_boxes[:,1].copy().astype(np.int32)\n        ex = total_boxes[:,2].copy().astype(np.int32)\n        ey = total_boxes[:,3].copy().astype(np.int32)\n\n        tmp = np.where(ex>w)\n        edx.flat[tmp] = np.expand_dims(-ex[tmp]+w+tmpw[tmp],1)\n        ex[tmp] = w\n        \n        tmp = np.where(ey>h)\n        edy.flat[tmp] = np.expand_dims(-ey[tmp]+h+tmph[tmp],1)\n        ey[tmp] = h\n\n        tmp = np.where(x<1)\n        dx.flat[tmp] = np.expand_dims(2-x[tmp],1)\n        x[tmp] = 1\n\n        tmp = np.where(y<1)\n        dy.flat[tmp] = np.expand_dims(2-y[tmp],1)\n        y[tmp] = 1\n        \n        return dy, edy, dx, edx, y, ey, x, ex, tmpw, tmph\n\n    # function [bboxA] = rerec(bboxA)\n    @staticmethod\n    def rerec(bboxA):\n        # convert bboxA to square\n        h = bboxA[:,3]-bboxA[:,1]\n        w = bboxA[:,2]-bboxA[:,0]\n        l = np.maximum(w, h)\n        bboxA[:,0] = bboxA[:,0]+w*0.5-l*0.5\n        bboxA[:,1] = bboxA[:,1]+h*0.5-l*0.5\n        bboxA[:,2:4] = bboxA[:,0:2] + np.transpose(np.tile(l,(2,1)))\n        return bboxA\n\n    def detect_faces(self,img):\n        # im: input image\n        # threshold: threshold=[th1 th2 th3], th1-3 are three steps's threshold\n        threshold = [ 0.6, 0.7, 0.9 ]  # three steps's threshold\n        # fastresize: resize img from last scale (using in high-resolution images) if fastresize==true\n        factor = 0.709 # scale factor\n        factor_count=0\n        total_boxes=np.empty((0,9))\n        points=np.array([])\n        h=img.shape[0]\n        w=img.shape[1]\n        minl=np.amin([h, w])\n        m=12.0/self.minsize\n        minl=minl*m\n        # creat scale pyramid\n        scales=[]\n        while minl>=12:\n            scales += [m*np.power(factor, factor_count)]\n            minl = minl*factor\n            factor_count += 1\n\n        # first stage\n        #t=time.time()\n        for j in range(len(scales)):\n            scale=scales[j]\n            hs=int(np.ceil(h*scale))\n            ws=int(np.ceil(w*scale))\n            im_data = cv2.resize(img, (ws,hs), interpolation=cv2.INTER_AREA)\n            im_data = (im_data-127.5)*0.0078125\n            img_x = np.expand_dims(im_data, 0)\n            img_y = np.transpose(img_x, (0,2,1,3))\n            out = self.pnet(img_y)\n            out0 = np.transpose(out[0], (0,2,1,3))\n            out1 = np.transpose(out[1], (0,2,1,3))\n            \n            boxes, _ = FacialImageProcessing.generateBoundingBox(out1[0,:,:,1].copy(), out0[0,:,:,:].copy(), scale, threshold[0])\n            \n            # inter-scale nms\n            pick = FacialImageProcessing.nms(boxes.copy(), 0.5, 'Union')\n            if boxes.size>0 and pick.size>0:\n                boxes = boxes[pick,:]\n                total_boxes = np.append(total_boxes, boxes, axis=0)\n        numbox = total_boxes.shape[0]\n        #elapsed = time.time() - t\n        #print('1 phase nb=%d elapsed=%f'%(numbox,elapsed))\n        if numbox>0:\n            pick = FacialImageProcessing.nms(total_boxes.copy(), 0.7, 'Union')\n            total_boxes = total_boxes[pick,:]\n            regw = total_boxes[:,2]-total_boxes[:,0]\n            regh = total_boxes[:,3]-total_boxes[:,1]\n            qq1 = total_boxes[:,0]+total_boxes[:,5]*regw\n            qq2 = total_boxes[:,1]+total_boxes[:,6]*regh\n            qq3 = total_boxes[:,2]+total_boxes[:,7]*regw\n            qq4 = total_boxes[:,3]+total_boxes[:,8]*regh\n            total_boxes = np.transpose(np.vstack([qq1, qq2, qq3, qq4, total_boxes[:,4]]))\n            total_boxes = FacialImageProcessing.rerec(total_boxes.copy())\n            total_boxes[:,0:4] = np.fix(total_boxes[:,0:4]).astype(np.int32)\n            dy, edy, dx, edx, y, ey, x, ex, tmpw, tmph = FacialImageProcessing.pad(total_boxes.copy(), w, h)\n\n        numbox = total_boxes.shape[0]\n        #elapsed = time.time() - t\n        #print('2 phase nb=%d elapsed=%f'%(numbox,elapsed))\n        if numbox>0:\n            # second stage\n            tempimg = np.zeros((24,24,3,numbox))\n            for k in range(0,numbox):\n                tmp = np.zeros((int(tmph[k]),int(tmpw[k]),3))\n                tmp[dy[k]-1:edy[k],dx[k]-1:edx[k],:] = img[y[k]-1:ey[k],x[k]-1:ex[k],:]\n                if tmp.shape[0]>0 and tmp.shape[1]>0 or tmp.shape[0]==0 and tmp.shape[1]==0:\n                    tempimg[:,:,:,k] = cv2.resize(tmp, (24,24), interpolation=cv2.INTER_AREA)\n                else:\n                    return np.empty()\n            tempimg = (tempimg-127.5)*0.0078125\n            tempimg1 = np.transpose(tempimg, (3,1,0,2))\n            out = self.rnet(tempimg1)\n            out0 = np.transpose(out[0])\n            out1 = np.transpose(out[1])\n            score = out1[1,:]\n            ipass = np.where(score>threshold[1])\n            total_boxes = np.hstack([total_boxes[ipass[0],0:4].copy(), np.expand_dims(score[ipass].copy(),1)])\n            mv = out0[:,ipass[0]]\n            if total_boxes.shape[0]>0:\n                pick = FacialImageProcessing.nms(total_boxes, 0.7, 'Union')\n                total_boxes = total_boxes[pick,:]\n                total_boxes = FacialImageProcessing.bbreg(total_boxes.copy(), np.transpose(mv[:,pick]))\n                total_boxes = FacialImageProcessing.rerec(total_boxes.copy())\n\n        numbox = total_boxes.shape[0]\n        #elapsed = time.time() - t\n        #print('3 phase nb=%d elapsed=%f'%(numbox,elapsed))\n        if numbox>0:\n            # third stage\n            total_boxes = np.fix(total_boxes).astype(np.int32)\n            dy, edy, dx, edx, y, ey, x, ex, tmpw, tmph = FacialImageProcessing.pad(total_boxes.copy(), w, h)\n            tempimg = np.zeros((48,48,3,numbox))\n            for k in range(0,numbox):\n                tmp = np.zeros((int(tmph[k]),int(tmpw[k]),3))\n                tmp[dy[k]-1:edy[k],dx[k]-1:edx[k],:] = img[y[k]-1:ey[k],x[k]-1:ex[k],:]\n                if tmp.shape[0]>0 and tmp.shape[1]>0 or tmp.shape[0]==0 and tmp.shape[1]==0:\n                    tempimg[:,:,:,k] = cv2.resize(tmp, (48,48), interpolation=cv2.INTER_AREA)\n                else:\n                    return np.empty()\n            tempimg = (tempimg-127.5)*0.0078125\n            tempimg1 = np.transpose(tempimg, (3,1,0,2))\n            out = self.onet(tempimg1)\n            out0 = np.transpose(out[0])\n            out1 = np.transpose(out[1])\n            out2 = np.transpose(out[2])\n            score = out2[1,:]\n            points = out1\n            ipass = np.where(score>threshold[2])\n            points = points[:,ipass[0]]\n            total_boxes = np.hstack([total_boxes[ipass[0],0:4].copy(), np.expand_dims(score[ipass].copy(),1)])\n            mv = out0[:,ipass[0]]\n\n            w = total_boxes[:,2]-total_boxes[:,0]+1\n            h = total_boxes[:,3]-total_boxes[:,1]+1\n            points[0:5,:] = np.tile(w,(5, 1))*points[0:5,:] + np.tile(total_boxes[:,0],(5, 1))-1\n            points[5:10,:] = np.tile(h,(5, 1))*points[5:10,:] + np.tile(total_boxes[:,1],(5, 1))-1\n            if total_boxes.shape[0]>0:\n                total_boxes = FacialImageProcessing.bbreg(total_boxes.copy(), np.transpose(mv))\n                pick = FacialImageProcessing.nms(total_boxes.copy(), 0.7, 'Min')\n                total_boxes = total_boxes[pick,:]\n                points = points[:,pick]\n        #elapsed = time.time() - t\n        #print('4 phase elapsed=%f'%(elapsed))            \n        return total_boxes, points","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2024-01-13T14:00:29.247717Z","iopub.execute_input":"2024-01-13T14:00:29.248141Z","iopub.status.idle":"2024-01-13T14:00:29.332305Z","shell.execute_reply.started":"2024-01-13T14:00:29.248107Z","shell.execute_reply":"2024-01-13T14:00:29.331483Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport cv2\nimport math\nfrom skimage import transform as trans\ndef get_iou(bb1, bb2):\n    \"\"\"\n    Calculate the Intersection over Union (IoU) of two bounding boxes.\n\n    Parameters\n    ----------\n    bb1 : array\n        order: {'x1', 'y1', 'x2', 'y2'}\n        The (x1, y1) position is at the top left corner,\n        the (x2, y2) position is at the bottom right corner\n    bb2 : array\n        order: {'x1', 'y1', 'x2', 'y2'}\n        The (x1, y1) position is at the top left corner,\n        the (x2, y2) position is at the bottom right corner\n\n    Returns\n    -------\n    float\n        in [0, 1]\n    \"\"\"\n\n    # determine the coordinates of the intersection rectangle\n    x_left = max(bb1[0], bb2[0])\n    y_top = max(bb1[1], bb2[1])\n    x_right = min(bb1[2], bb2[2])\n    y_bottom = min(bb1[3], bb2[3])\n\n    if x_right < x_left or y_bottom < y_top:\n        return 0.0\n\n    # The intersection of two axis-aligned bounding boxes is always an\n    # axis-aligned bounding box\n    intersection_area = (x_right - x_left) * (y_bottom - y_top)\n\n    # compute the area of both AABBs\n    bb1_area = (bb1[2] - bb1[0]) * (bb1[3] - bb1[1])\n    bb2_area = (bb2[2] - bb2[0]) * (bb2[3] - bb2[1])\n\n    # compute the intersection over union by taking the intersection\n    # area and dividing it by the sum of prediction + ground-truth\n    # areas - the interesection area\n    iou = intersection_area / float(bb1_area + bb2_area - intersection_area)\n    return iou\n\n#print(get_iou([10,10,20,20],[15,15,25,25]))\n\ndef preprocess(img, bbox=None, landmark=None, **kwargs):\n    M = None\n    image_size = [224,224]\n    src = np.array([\n      [30.2946, 51.6963],\n      [65.5318, 51.5014],\n      [48.0252, 71.7366],\n      [33.5493, 92.3655],\n      [62.7299, 92.2041] ], dtype=np.float32 )\n    if image_size[1]==224:\n        src[:,0] += 8.0\n    src*=2\n    if landmark is not None:\n        dst = landmark.astype(np.float32)\n\n        tform = trans.SimilarityTransform()\n        tform.estimate(dst, src)\n        M = tform.params[0:2,:]\n\n    if M is None:\n        if bbox is None:\n            det = np.zeros(4, dtype=np.int32)\n            det[0] = int(img.shape[1]*0.0625)\n            det[1] = int(img.shape[0]*0.0625)\n            det[2] = img.shape[1] - det[0]\n            det[3] = img.shape[0] - det[1]\n        else:\n            det = bbox\n        margin = kwargs.get('margin', 44)\n        bb = np.zeros(4, dtype=np.int32)\n        bb[0] = np.maximum(det[0]-margin//2, 0)\n        bb[1] = np.maximum(det[1]-margin//2, 0)\n        bb[2] = np.minimum(det[2]+margin//2, img.shape[1])\n        bb[3] = np.minimum(det[3]+margin//2, img.shape[0])\n        ret = img[bb[1]:bb[3],bb[0]:bb[2],:]\n        if len(image_size)>0:\n              ret = cv2.resize(ret, (image_size[1], image_size[0]))\n        return ret \n    else: #do align using landmark\n        assert len(image_size)==2\n        warped = cv2.warpAffine(img,M,(image_size[1],image_size[0]), borderValue = 0.0)\n        return warped","metadata":{"execution":{"iopub.status.busy":"2024-01-13T14:02:28.717892Z","iopub.execute_input":"2024-01-13T14:02:28.718743Z","iopub.status.idle":"2024-01-13T14:02:29.179461Z","shell.execute_reply.started":"2024-01-13T14:02:28.718708Z","shell.execute_reply":"2024-01-13T14:02:29.178419Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def predict_landmarks(img, bbox):\n    face_region = img[bbox[1]:bbox[3], bbox[0]:bbox[2]]\n    landmarks = imgProcessing.detect_faces(face_region)[1]\n    return landmarks","metadata":{"execution":{"iopub.status.busy":"2024-01-13T14:03:27.101893Z","iopub.execute_input":"2024-01-13T14:03:27.102573Z","iopub.status.idle":"2024-01-13T14:03:27.107723Z","shell.execute_reply.started":"2024-01-13T14:03:27.102537Z","shell.execute_reply":"2024-01-13T14:03:27.106745Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nfpath='/kaggle/input/fac-data-p1/data/mnt/md0/projects/sami-hackathon/private/data/10004189.jpg'\nframe_bgr=cv2.imread(fpath)\nplt.figure(figsize=(5, 5))\nframe = cv2.cvtColor(frame_bgr, cv2.COLOR_BGR2RGB)\nplt.imshow(frame)\nimgProcessing=FacialImageProcessing(False)\nbounding_boxes, points = imgProcessing.detect_faces(frame)\npoints = points.T\nfor bbox,p in zip(bounding_boxes, points):\n    box = bbox.astype(int) #x1, y1, x2, y2\n    p = p.reshape((2,5)).T\n    \n    plt.figure(figsize=(5, 5))\n    fig, (ax1, ax2) = plt.subplots(1, 2)\n    \n    face_img=preprocess(frame,box,None) #không truyền points (landmark) vào\n    ax1.set_title('Cropped')\n    ax1.imshow(face_img)\n    \n    face_img=preprocess(frame,box,p) #có truyền\n    ax2.set_title('Aligned')\n    ax2.imshow(face_img)","metadata":{"execution":{"iopub.status.busy":"2024-01-13T14:19:02.935873Z","iopub.execute_input":"2024-01-13T14:19:02.936991Z","iopub.status.idle":"2024-01-13T14:19:03.825966Z","shell.execute_reply.started":"2024-01-13T14:19:02.936953Z","shell.execute_reply":"2024-01-13T14:19:03.82509Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\ndef save_aligned_faces(source_path,save_path, dataframe):\n    np_images = []\n    if not os.path.exists(save_path):\n        os.mkdir(save_path)\n    for index, row in tqdm(dataframe.iterrows(), desc=\"Processing images\"):\n        image = row['file_name']\n        prev_b = None\n        counter = 0\n#         for image in sorted(os.listdir(os.path.join(source_path, folder))):\n        filename = os.path.join(source_path, image)\n        frame = cv2.imread(filename)\n        frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n        bounding_boxes, points = imgProcessing.detect_faces(frame)\n        points = points.T\n        best_ind=None\n        if len(bounding_boxes)==0:\n            print('No faces found for ',filename)\n            counter+=1\n            if prev_b is None or counter>3:\n                continue\n            else:\n                b=prev_b\n        elif len(bounding_boxes)>1:\n            print('Too many faces (',len(bounding_boxes),') found for ',filename)\n            if prev_b is None:\n                #continue\n                best_ind=0\n                b=[int(bi) for bi in bounding_boxes[best_ind]]\n                counter=0\n            else:\n                best_iou=0\n                for i in range(len(bounding_boxes)):\n                    iou=get_iou(bounding_boxes[i],prev_b)\n                    if iou>best_iou:\n                        best_iou=iou\n                        best_ind=i\n                if best_iou>0:\n                    b=[int(bi) for bi in bounding_boxes[best_ind]]\n                    print('best_iou (',best_iou,') best_bb ',bounding_boxes[best_ind])\n                else:\n                    best_ind=0\n                    b=[int(bi) for bi in bounding_boxes[best_ind]]\n                    counter=0\n        else:\n            best_ind=0\n            b=[int(bi) for bi in bounding_boxes[best_ind]]\n            counter=0\n        prev_b=b\n\n        if True:\n            p=None\n            if best_ind is not None:\n                p=points[best_ind]\n                if True: #not USE_RETINA_FACE:\n                    p = p.reshape((2,5)).T\n            face_img=preprocess(frame,b,None) #p)\n        else:\n            x1,y1,x2,y2=b[0:4]\n            face_img=frame[y1:y2,x1:x2,:]\n        if np.prod(face_img.shape)==0:\n            print('Empty face ',b,' found for ',filename)\n            bb = eval(row['bbox'])\n            y1 = int(bb[1])\n            y2 = int(bb[1] + bb[3])\n            x1 = int(bb[0])\n            x2 = int(bb[0] + bb[2])\n            face_img = frame[y1:y2, x1:x2]\n            \n        np_images.append(face_img)\n    np.save('np_images.npy', np_images)\n\n#aligned\nsave_aligned_faces('/kaggle/input/fac-data-p1/data/mnt/md0/projects/sami-hackathon/private/data/','npy_aligned_images', df)\n","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2024-01-13T14:39:57.913852Z","iopub.execute_input":"2024-01-13T14:39:57.914758Z","iopub.status.idle":"2024-01-13T16:15:08.216297Z","shell.execute_reply.started":"2024-01-13T14:39:57.914721Z","shell.execute_reply":"2024-01-13T16:15:08.215471Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}