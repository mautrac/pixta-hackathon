{"cells":[{"cell_type":"code","execution_count":2,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2024-01-19T14:05:08.349863Z","iopub.status.busy":"2024-01-19T14:05:08.349100Z","iopub.status.idle":"2024-01-19T14:05:37.772058Z","shell.execute_reply":"2024-01-19T14:05:37.770828Z","shell.execute_reply.started":"2024-01-19T14:05:08.349832Z"},"trusted":true},"outputs":[],"source":["!pip -q install deepface\n","!pip -q install ultralytics"]},{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2024-01-19T14:07:04.255025Z","iopub.status.busy":"2024-01-19T14:07:04.254266Z","iopub.status.idle":"2024-01-19T14:07:07.182535Z","shell.execute_reply":"2024-01-19T14:07:07.181751Z","shell.execute_reply.started":"2024-01-19T14:07:04.254980Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/torchvision/datapoints/__init__.py:12: UserWarning: The torchvision.datapoints and torchvision.transforms.v2 namespaces are still Beta. While we do not expect major breaking changes, some APIs may still change according to user feedback. Please submit any feedback you may have in this issue: https://github.com/pytorch/vision/issues/6753, and you can also check out https://github.com/pytorch/vision/issues/7319 to learn more about the APIs that we suspect might involve future changes. You can silence this warning by calling torchvision.disable_beta_transforms_warning().\n","  warnings.warn(_BETA_TRANSFORMS_WARNING)\n","/opt/conda/lib/python3.10/site-packages/torchvision/transforms/v2/__init__.py:54: UserWarning: The torchvision.datapoints and torchvision.transforms.v2 namespaces are still Beta. While we do not expect major breaking changes, some APIs may still change according to user feedback. Please submit any feedback you may have in this issue: https://github.com/pytorch/vision/issues/6753, and you can also check out https://github.com/pytorch/vision/issues/7319 to learn more about the APIs that we suspect might involve future changes. You can silence this warning by calling torchvision.disable_beta_transforms_warning().\n","  warnings.warn(_BETA_TRANSFORMS_WARNING)\n","/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.3\n","  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"]}],"source":["import pandas as pd\n","import numpy as np\n","\n","import cv2\n","import matplotlib.pyplot as plt\n","\n","import torch\n","import torch.nn as nn\n","\n","from tqdm import tqdm\n","\n","import torch\n","from PIL import Image\n","from torchvision.transforms import v2\n","from torchvision import transforms\n","\n","import os\n","\n","from sklearn import preprocessing"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2024-01-19T14:07:07.184423Z","iopub.status.busy":"2024-01-19T14:07:07.183998Z","iopub.status.idle":"2024-01-19T14:07:07.212806Z","shell.execute_reply":"2024-01-19T14:07:07.211867Z","shell.execute_reply.started":"2024-01-19T14:07:07.184397Z"},"trusted":true},"outputs":[],"source":["backends = [\n","  'opencv', \n","  'ssd', \n","  'dlib', \n","  'mtcnn', \n","  'retinaface', \n","  'mediapipe',\n","  'yolov8',\n","  'yunet',\n","  'fastmtcnn',\n","]\n","\n","if torch.cuda.is_available():\n","    device = 'cuda:0'\n","else:\n","    device = 'cpu'"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2024-01-19T14:07:11.310555Z","iopub.status.busy":"2024-01-19T14:07:11.310187Z","iopub.status.idle":"2024-01-19T14:07:11.437460Z","shell.execute_reply":"2024-01-19T14:07:11.436599Z","shell.execute_reply.started":"2024-01-19T14:07:11.310527Z"},"trusted":true},"outputs":[],"source":["data = pd.read_csv(\"/kaggle/input/fac-data-p1/labels.csv\")\n","path = \"/kaggle/input/fac-data-p1/data/mnt/md0/projects/sami-hackathon/private/data/\"\n","cols = data.columns\n","y_raw = data[cols[4:]].values\n","files_list = data['file_name'].values"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2024-01-19T14:07:54.822673Z","iopub.status.busy":"2024-01-19T14:07:54.822311Z","iopub.status.idle":"2024-01-19T14:07:55.172463Z","shell.execute_reply":"2024-01-19T14:07:55.171721Z","shell.execute_reply.started":"2024-01-19T14:07:54.822643Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["{'age': ['Baby', 'Kid', 'Teenager', '20-30s', '40-50s', 'Senior'], 'race': array(['Caucasian', 'Mongoloid', 'Negroid'], dtype=object), 'masked': array(['unmasked', 'masked'], dtype=object), 'skintone': array(['mid-light', 'light', 'mid-dark', 'dark'], dtype=object), 'emotion': array(['Neutral', 'Happiness', 'Anger', 'Surprise', 'Fear', 'Sadness',\n","       'Disgust'], dtype=object), 'gender': array(['Male', 'Female'], dtype=object)}\n","[array(['Baby', 'Kid', 'Teenager', '20-30s', '40-50s', 'Senior'],\n","      dtype=object)]\n","[array(['Caucasian', 'Mongoloid', 'Negroid'], dtype=object)]\n","[array(['unmasked', 'masked'], dtype=object)]\n","[array(['mid-light', 'light', 'mid-dark', 'dark'], dtype=object)]\n","[array(['Neutral', 'Happiness', 'Anger', 'Surprise', 'Fear', 'Sadness',\n","       'Disgust'], dtype=object)]\n","[array(['Male', 'Female'], dtype=object)]\n"]}],"source":["labels_set = {}\n","for col in cols[4:]:\n","    temp = data[col].unique()\n","    labels_set[col] = temp\n","    \n","labels_set['age'] = ['Baby', 'Kid', 'Teenager', '20-30s', '40-50s', 'Senior']\n","print(labels_set)\n","\n","labels = dict()\n","for col in cols[4:]:\n","    enc = preprocessing.OneHotEncoder(categories=[labels_set[col]]).fit(data[col].values.reshape(-1 ,1))\n","    labels[col] = enc.transform(data[col].values.reshape(-1 ,1)).toarray()\n","    print(enc.categories_)\n","\n","temp = data[['file_name', 'bbox']].values\n","file_names = temp[:, 0]\n","bboxs = temp[:, 1]\n","bbxs = []\n","for i in range(len(bboxs)):\n","    bbxs.append(eval(bboxs[i]))\n","bboxes = np.array(bbxs, int)\n","    "]},{"cell_type":"code","execution_count":50,"metadata":{"execution":{"iopub.execute_input":"2024-01-19T14:40:20.884923Z","iopub.status.busy":"2024-01-19T14:40:20.884015Z","iopub.status.idle":"2024-01-19T14:40:20.890887Z","shell.execute_reply":"2024-01-19T14:40:20.889619Z","shell.execute_reply.started":"2024-01-19T14:40:20.884879Z"},"trusted":true},"outputs":[],"source":["labels_set['age'] = ['20-30s', '40-50s', 'Baby', 'Kid', 'Senior', 'Teenager']\n","labels_set['emotion']= ['Anger', 'Disgust', 'Fear', 'Happiness', 'Neutral', 'Sadness', 'Surprise']\n","#labels_set['age'] = {0: '20-30s', 1: '40-50s', 3: 'Kid', 4: 'Senior', 2: 'Baby', 5: 'Teenager'}\n","#labels_set['emotion'] = {4: 'Neutral', 3: 'Happiness', 0: 'Anger', 6: 'Surprise', 2: 'Fear', 5: 'Sadness', 1: 'Disgust'}"]},{"cell_type":"code","execution_count":10,"metadata":{"execution":{"iopub.execute_input":"2024-01-19T14:09:40.831126Z","iopub.status.busy":"2024-01-19T14:09:40.830292Z","iopub.status.idle":"2024-01-19T14:09:40.840159Z","shell.execute_reply":"2024-01-19T14:09:40.839245Z","shell.execute_reply.started":"2024-01-19T14:09:40.831092Z"},"trusted":true},"outputs":[],"source":["\n","class Dataset(torch.utils.data.Dataset):\n","\n","    def __init__(self, list_files, bboxes, labels, root):\n","        self.labels = labels\n","        self.list_files = list_files\n","        self.bboxes = bboxes\n","        self.root = root\n","\n","\n","    def __len__(self):\n","        return len(self.list_files)\n","\n","\n","    def __getitem__(self, index):\n","        name = self.list_files[index]\n","\n","        X = Image.open(self.root + '/' + name).convert('RGB')\n","        if self.bboxes is not None:\n","            X = X.crop((self.bboxes[index][0], self.bboxes[index][1], self.bboxes[index][0] + self.bboxes[index][2], \\\n","                       self.bboxes[index][1] + self.bboxes[index][3]))\n","\n","        y = dict()\n","        y[\"age\"] = labels['age'][index]\n","        y[\"race\"] = labels['race'][index]\n","        y[\"masked\"] = labels['masked'][index]\n","        y[\"skintone\"] = labels['skintone'][index]\n","        y[\"emotion\"] = labels['emotion'][index]\n","        y[\"gender\"] = labels['gender'][index]\n","\n","        X = mytransform(X)\n","\n","        return X, y\n"]},{"cell_type":"markdown","metadata":{},"source":["# Model"]},{"cell_type":"code","execution_count":23,"metadata":{"execution":{"iopub.execute_input":"2024-01-19T14:19:51.464459Z","iopub.status.busy":"2024-01-19T14:19:51.464115Z","iopub.status.idle":"2024-01-19T14:19:51.508793Z","shell.execute_reply":"2024-01-19T14:19:51.507824Z","shell.execute_reply.started":"2024-01-19T14:19:51.464432Z"},"trusted":true},"outputs":[],"source":["import torch.nn.functional as F\n","\n","class BasicConv2d(nn.Module):\n","    def __init__(self, in_channels, out_channels, **kwargs):\n","        super(BasicConv2d, self).__init__()\n","        self.conv = nn.Conv2d(in_channels, out_channels, bias=False, **kwargs)\n","        self.bn = nn.BatchNorm2d(out_channels, eps=0.001)\n","\n","    def forward(self, x):\n","        x = self.conv(x)\n","        x = self.bn(x)\n","        return F.relu(x, inplace=True)\n","\n","class InceptionBlock(nn.Module):\n","    def __init__(\n","        self, \n","        in_channels, \n","        out_1x1,\n","        red_3x3,\n","        out_3x3,\n","        red_5x5,\n","        out_5x5,\n","        out_pool,\n","    ):\n","        super(InceptionBlock, self).__init__()\n","        self.branch1 = BasicConv2d(in_channels, out_1x1, kernel_size=1)\n","        self.branch2 = nn.Sequential(\n","            BasicConv2d(in_channels, red_3x3, kernel_size=1, padding=0),\n","            BasicConv2d(red_3x3, out_3x3, kernel_size=3, padding=1),\n","        )\n","        self.branch3 = nn.Sequential(\n","            BasicConv2d(in_channels, red_5x5, kernel_size=1),\n","            BasicConv2d(red_5x5, out_5x5, kernel_size=5, padding=2),\n","        )\n","        self.branch4 = nn.Sequential(\n","            nn.MaxPool2d(kernel_size=3, padding=1, stride=1),\n","            BasicConv2d(in_channels, out_pool, kernel_size=1),\n","        )\n","    \n","    def forward(self, x):\n","        branches = (self.branch1, self.branch2, self.branch3, self.branch4)\n","        return torch.cat([branch(x) for branch in branches], 1)\n","\n","    \n","class SpatialAttention(nn.Module):\n","    def __init__(self, kernel_size=7):\n","        super(SpatialAttention, self).__init__()\n","\n","        self.conv1 = nn.Conv2d(2, 1, kernel_size, padding=kernel_size//2, bias=False)\n","        self.sigmoid = nn.Sigmoid()\n","\n","    def forward(self, x):\n","        avg_out = torch.mean(x, dim=1, keepdim=True)\n","        max_out, _ = torch.max(x, dim=1, keepdim=True)\n","        x = torch.cat([avg_out, max_out], dim=1)\n","        x = self.conv1(x)\n","        return self.sigmoid(x)\n","    \n","\n","class SkintoneRaceModel(nn.Module):\n","    def __init__(self):\n","        super(SkintoneRaceModel, self).__init__()\n","        self.conv5x5x48_1 = BasicConv2d(3, 48, kernel_size = (5, 5), stride = 2, padding = 2)\n","        self.conv3x3x96_1 = BasicConv2d(48, 96, kernel_size = (3, 3), stride = 1, padding = 1)\n","        self.conv3x3x192_1 = BasicConv2d(96, 192, kernel_size = (3, 3), stride = 1, padding = 1)\n","        self.conv3x3x384 = BasicConv2d(192, 384, kernel_size = (3, 3), stride = 1, padding = 1)\n","        \n","        self.conv5x5x48 = BasicConv2d(3, 48, kernel_size = (5, 5), stride = 2, padding = 1)\n","        self.conv3x3x96 = BasicConv2d(48, 96, kernel_size = (3, 3), stride = 1, padding = 1)\n","        self.conv3x3x192 = BasicConv2d(96, 192, kernel_size = (3, 3), stride = 1, padding = 1)\n","        \n","        self.inception1 = InceptionBlock(192, 128, 128, 256, 128, 256, 128)\n","        self.conv3x3x512 = BasicConv2d(128 + 256 + 256 + 128, 512, kernel_size = (3, 3), stride = 1, padding = 1)\n","        \n","        self.gap = torch.nn.AdaptiveAvgPool2d(1)\n","        \n","        self.fc_race = nn.Linear(512, 3)\n","        self.fc_skintone = nn.Linear(384, 4)\n","        self.dropout = nn.Dropout(0.3)\n","        \n","    def forward(self, x):\n","        x_skin = self.conv5x5x48_1(x)\n","        x_skin = nn.functional.max_pool2d(x_skin, (2, 2), 2)\n","        \n","        x_skin = self.conv3x3x96_1(x_skin)\n","        x_skin = nn.functional.max_pool2d(x_skin, (2, 2), 2)\n","        \n","        x_skin = self.conv3x3x192_1(x_skin)\n","        x_skin = nn.functional.max_pool2d(x_skin, (2, 2), 2)\n","        \n","        x_skin = self.conv3x3x384(x_skin)\n","        x_skin = nn.functional.max_pool2d(x_skin, (2, 2), 2)\n","        \n","        N, C, W, H = x_skin.shape\n","        x_skin = self.gap(x_skin).view(N, -1)\n","        x_skin = self.dropout(x_skin)\n","        x_skin = self.fc_skintone(x_skin)\n","        x_skin = nn.functional.softmax(x_skin, dim = 1)\n","            \n","        #  RACE\n","        x_race = self.conv5x5x48(x)\n","        x_race = nn.functional.max_pool2d(x_race, (2, 2), 2)\n","        \n","        x_race = self.conv3x3x96(x_race)\n","        x_race = nn.functional.max_pool2d(x_race, (2, 2), 2)\n","        \n","        x_race = self.conv3x3x192(x_race)\n","        x_race = nn.functional.max_pool2d(x_race, (2, 2), 2)\n","        \n","        x_race = self.inception1(x_race)\n","        x_race = nn.functional.max_pool2d(x_race, (2, 2), 2)\n","        # 1152 x 14 x 14\n","        \n","        x_race = self.conv3x3x512(x_race)\n","        x_race = nn.functional.max_pool2d(x_race, (2, 2), 2)\n","        \n","        N, C, W, H = x_race.shape\n","        #print(x.shape)\n","        #x = x.view(N, -1)\n","        x_race = self.gap(x_race).view(N, -1)\n","        x_race = self.dropout(x_race)\n","        x_race = self.fc_race(x_race)\n","        x_race = nn.functional.softmax(x_race, dim = 1)\n","        \n","        return {\"race\": x_race, \"skintone\": x_skin}\n","    \n","    \n","class MaskedModel(nn.Module):\n","    def __init__(self,  att_out = False):\n","        super(MaskedModel, self).__init__()\n","        self.att_out = att_out\n","        self.conv5x5x3 = BasicConv2d(3, 3, kernel_size = (5, 5), stride = 1, padding = 2)\n","        self.conv3x3x96 = BasicConv2d(3, 96, kernel_size = (3, 3), stride = 1, padding = 1)\n","        self.conv3x3x192 = BasicConv2d(96, 192, kernel_size = (3, 3), stride = 1, padding = 1)\n","        self.conv3x3x256 = BasicConv2d(192, 256, kernel_size = (3, 3), stride = 1, padding = 1)\n","        self.inception1 = InceptionBlock(256, 192, 128, 384, 96, 384, 192)\n","        \n","        self.spatial_module = SpatialAttention()\n","        \n","        self.gap = torch.nn.AdaptiveAvgPool2d(1)\n","        self.fc = nn.Linear(192 + 384 + 384 + 192, 2)\n","        self.dropout = nn.Dropout(0.3)\n","        \n","    def forward(self, x):\n","        #x: 3 x 224 x 224\n","        x = self.conv5x5x3(x)\n","        x = nn.functional.max_pool2d(x, (2, 2), 2)\n","        \n","        x = self.conv3x3x96(x)\n","        x = nn.functional.max_pool2d(x, (2, 2), 2)\n","        \n","        x = self.conv3x3x192(x)\n","        x = nn.functional.max_pool2d(x, (2, 2), 2)\n","        \n","        x = self.conv3x3x256(x)\n","        x = nn.functional.max_pool2d(x, (2, 2), 2)\n","        \n","        x = self.inception1(x)\n","        x = nn.functional.max_pool2d(x, (2, 2), 2)\n","        \n","        N, C, W, H = x.shape\n","        sp_att = self.spatial_module(x).view(-1, 1, W, H)\n","\n","        sp_att2 = sp_att.expand(-1, C, W, H)\n","        x = x * sp_att2\n","        \n","        # 896 x 7 x 7\n","        x = self.gap(x).view(-1, C)\n","        x = self.dropout(x)\n","        \n","        x = self.fc(x)\n","        \n","        x = nn.functional.softmax(x, dim = 1)\n","        \n","        if self.att_out:\n","            return {\"masked\": x}, sp_att\n","        \n","        return {\"masked\": x}\n","    \n","\n","class GenderModel(nn.Module):\n","    def __init__(self):\n","        super(GenderModel, self).__init__()\n","        self.conv5x5x3 = BasicConv2d(3, 3, kernel_size = (5, 5), stride = 1, padding = 2)\n","        self.conv3x3x96 = BasicConv2d(3, 96, kernel_size = (3, 3), stride = 1, padding = 1)\n","        self.conv3x3x192 = BasicConv2d(96, 192, kernel_size = (3, 3), stride = 1, padding = 1)\n","        self.conv3x3x384 = BasicConv2d(192, 384, kernel_size = (3, 3), stride = 1, padding = 1)\n","        self.inception1 = InceptionBlock(384, 192, 128, 384, 128, 384, 192)\n","        \n","        self.gap = torch.nn.AdaptiveAvgPool2d(1)\n","        \n","        self.fc = nn.Linear(192 + 384 + 384 + 192, 2)\n","        self.dropout = nn.Dropout(0.3)\n","        \n","    def forward(self, x):\n","        #x: 3 x 224 x 224\n","        x = self.conv5x5x3(x)\n","        x = nn.functional.max_pool2d(x, (2, 2), 2)\n","        \n","        x = self.conv3x3x96(x)\n","        x = nn.functional.max_pool2d(x, (2, 2), 2)\n","        \n","        x = self.conv3x3x192(x)\n","        x = nn.functional.max_pool2d(x, (2, 2), 2)\n","        \n","        x = self.conv3x3x384(x)\n","        x = nn.functional.max_pool2d(x, (2, 2), 2)\n","        \n","        x = self.inception1(x)\n","        x = nn.functional.max_pool2d(x, (2, 2), 2)\n","        \n","        N, C, W, H = x.shape\n","        # 896 x 7 x 7\n","        x = self.gap(x).view(N, C)\n","        x = self.dropout(x)\n","        \n","        x = self.fc(x)\n","        \n","        x = nn.functional.softmax(x, dim = 1)\n","           \n","        return {\"gender\": x}"]},{"cell_type":"code","execution_count":46,"metadata":{"execution":{"iopub.execute_input":"2024-01-19T14:36:14.668339Z","iopub.status.busy":"2024-01-19T14:36:14.667931Z","iopub.status.idle":"2024-01-19T14:36:14.678472Z","shell.execute_reply":"2024-01-19T14:36:14.677570Z","shell.execute_reply.started":"2024-01-19T14:36:14.668307Z"},"trusted":true},"outputs":[],"source":["class WrapModel(nn.Module):\n","    def __init__(self):\n","        super(WrapModel, self).__init__()\n","        self.model_skin_race = SkintoneRaceModel()\n","        self.model_masked = MaskedModel()\n","        self.model_gender = GenderModel()\n","        self.model_age = torch.load('/kaggle/input/fac-model-vit/model_age.pth')\n","        self.model_emotion = torch.load('/kaggle/input/fac-model-vit/model_emotion.pth')\n","        \n","        \n","        self.model_skin_race.load_state_dict(torch.load('/kaggle/input/pixta-model2/skintone_race_model_epoch_49'))\n","        self.model_masked.load_state_dict(torch.load('/kaggle/input/model-pixta/masked_model_epoch_49'))\n","        self.model_gender.load_state_dict(torch.load('/kaggle/input/pixta-model2/gender_model_epoch_49'))\n","        \n","        self.model_skin_race.to(device)\n","        self.model_masked.to(device)\n","        self.model_gender.to(device)\n","        self.model_age.to(device)\n","        self.model_emotion.to(device)\n","        \n","        self.model_skin_race.eval()\n","        self.model_masked.eval()\n","        self.model_gender.eval()\n","        self.model_age.eval()\n","        self.model_emotion.eval()\n","        \n","    def forward(self, x):\n","        age = self.model_age(x)\n","        gender = self.model_gender(x)\n","        skin_race = self.model_skin_race(x)\n","        masked = self.model_masked(x)\n","        \n","        emotion = self.model_emotion(x)\n","        age = self.model_age(x)\n","        \n","        return gender | skin_race | masked | masked | {'emotion': emotion} | {'age': age}\n","    "]},{"cell_type":"markdown","metadata":{},"source":["# Run"]},{"cell_type":"code","execution_count":47,"metadata":{"collapsed":true,"execution":{"iopub.execute_input":"2024-01-19T14:36:16.705265Z","iopub.status.busy":"2024-01-19T14:36:16.704393Z","iopub.status.idle":"2024-01-19T14:36:17.445596Z","shell.execute_reply":"2024-01-19T14:36:17.444689Z","shell.execute_reply.started":"2024-01-19T14:36:16.705229Z"},"jupyter":{"outputs_hidden":true},"trusted":true},"outputs":[{"data":{"text/plain":["WrapModel(\n","  (model_skin_race): SkintoneRaceModel(\n","    (conv5x5x48_1): BasicConv2d(\n","      (conv): Conv2d(3, 48, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), bias=False)\n","      (bn): BatchNorm2d(48, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n","    )\n","    (conv3x3x96_1): BasicConv2d(\n","      (conv): Conv2d(48, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn): BatchNorm2d(96, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n","    )\n","    (conv3x3x192_1): BasicConv2d(\n","      (conv): Conv2d(96, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n","    )\n","    (conv3x3x384): BasicConv2d(\n","      (conv): Conv2d(192, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n","    )\n","    (conv5x5x48): BasicConv2d(\n","      (conv): Conv2d(3, 48, kernel_size=(5, 5), stride=(2, 2), padding=(1, 1), bias=False)\n","      (bn): BatchNorm2d(48, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n","    )\n","    (conv3x3x96): BasicConv2d(\n","      (conv): Conv2d(48, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn): BatchNorm2d(96, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n","    )\n","    (conv3x3x192): BasicConv2d(\n","      (conv): Conv2d(96, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n","    )\n","    (inception1): InceptionBlock(\n","      (branch1): BasicConv2d(\n","        (conv): Conv2d(192, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n","      )\n","      (branch2): Sequential(\n","        (0): BasicConv2d(\n","          (conv): Conv2d(192, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n","        )\n","        (1): BasicConv2d(\n","          (conv): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","          (bn): BatchNorm2d(256, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n","        )\n","      )\n","      (branch3): Sequential(\n","        (0): BasicConv2d(\n","          (conv): Conv2d(192, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n","        )\n","        (1): BasicConv2d(\n","          (conv): Conv2d(128, 256, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), bias=False)\n","          (bn): BatchNorm2d(256, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n","        )\n","      )\n","      (branch4): Sequential(\n","        (0): MaxPool2d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)\n","        (1): BasicConv2d(\n","          (conv): Conv2d(192, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n","        )\n","      )\n","    )\n","    (conv3x3x512): BasicConv2d(\n","      (conv): Conv2d(768, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn): BatchNorm2d(512, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n","    )\n","    (gap): AdaptiveAvgPool2d(output_size=1)\n","    (fc_race): Linear(in_features=512, out_features=3, bias=True)\n","    (fc_skintone): Linear(in_features=384, out_features=4, bias=True)\n","    (dropout): Dropout(p=0.3, inplace=False)\n","  )\n","  (model_masked): MaskedModel(\n","    (conv5x5x3): BasicConv2d(\n","      (conv): Conv2d(3, 3, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), bias=False)\n","      (bn): BatchNorm2d(3, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n","    )\n","    (conv3x3x96): BasicConv2d(\n","      (conv): Conv2d(3, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn): BatchNorm2d(96, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n","    )\n","    (conv3x3x192): BasicConv2d(\n","      (conv): Conv2d(96, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n","    )\n","    (conv3x3x256): BasicConv2d(\n","      (conv): Conv2d(192, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn): BatchNorm2d(256, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n","    )\n","    (inception1): InceptionBlock(\n","      (branch1): BasicConv2d(\n","        (conv): Conv2d(256, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n","      )\n","      (branch2): Sequential(\n","        (0): BasicConv2d(\n","          (conv): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n","        )\n","        (1): BasicConv2d(\n","          (conv): Conv2d(128, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","          (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n","        )\n","      )\n","      (branch3): Sequential(\n","        (0): BasicConv2d(\n","          (conv): Conv2d(256, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn): BatchNorm2d(96, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n","        )\n","        (1): BasicConv2d(\n","          (conv): Conv2d(96, 384, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), bias=False)\n","          (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n","        )\n","      )\n","      (branch4): Sequential(\n","        (0): MaxPool2d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)\n","        (1): BasicConv2d(\n","          (conv): Conv2d(256, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n","        )\n","      )\n","    )\n","    (spatial_module): SpatialAttention(\n","      (conv1): Conv2d(2, 1, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)\n","      (sigmoid): Sigmoid()\n","    )\n","    (gap): AdaptiveAvgPool2d(output_size=1)\n","    (fc): Linear(in_features=1152, out_features=2, bias=True)\n","    (dropout): Dropout(p=0.3, inplace=False)\n","  )\n","  (model_gender): GenderModel(\n","    (conv5x5x3): BasicConv2d(\n","      (conv): Conv2d(3, 3, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), bias=False)\n","      (bn): BatchNorm2d(3, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n","    )\n","    (conv3x3x96): BasicConv2d(\n","      (conv): Conv2d(3, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn): BatchNorm2d(96, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n","    )\n","    (conv3x3x192): BasicConv2d(\n","      (conv): Conv2d(96, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n","    )\n","    (conv3x3x384): BasicConv2d(\n","      (conv): Conv2d(192, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n","    )\n","    (inception1): InceptionBlock(\n","      (branch1): BasicConv2d(\n","        (conv): Conv2d(384, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n","      )\n","      (branch2): Sequential(\n","        (0): BasicConv2d(\n","          (conv): Conv2d(384, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n","        )\n","        (1): BasicConv2d(\n","          (conv): Conv2d(128, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","          (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n","        )\n","      )\n","      (branch3): Sequential(\n","        (0): BasicConv2d(\n","          (conv): Conv2d(384, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n","        )\n","        (1): BasicConv2d(\n","          (conv): Conv2d(128, 384, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), bias=False)\n","          (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n","        )\n","      )\n","      (branch4): Sequential(\n","        (0): MaxPool2d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)\n","        (1): BasicConv2d(\n","          (conv): Conv2d(384, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n","        )\n","      )\n","    )\n","    (gap): AdaptiveAvgPool2d(output_size=1)\n","    (fc): Linear(in_features=1152, out_features=2, bias=True)\n","    (dropout): Dropout(p=0.3, inplace=False)\n","  )\n","  (model_age): ViTForImageClassification(\n","    (vit): ViTModel(\n","      (embeddings): ViTEmbeddings(\n","        (patch_embeddings): ViTPatchEmbeddings(\n","          (projection): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n","        )\n","        (dropout): Dropout(p=0.0, inplace=False)\n","      )\n","      (encoder): ViTEncoder(\n","        (layer): ModuleList(\n","          (0-11): 12 x ViTLayer(\n","            (attention): ViTAttention(\n","              (attention): ViTSelfAttention(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.0, inplace=False)\n","              )\n","              (output): ViTSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.0, inplace=False)\n","              )\n","            )\n","            (intermediate): ViTIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","              (intermediate_act_fn): GELUActivation()\n","            )\n","            (output): ViTOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.0, inplace=False)\n","            )\n","            (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          )\n","        )\n","      )\n","      (layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","    )\n","    (classifier): Linear(in_features=768, out_features=6, bias=True)\n","  )\n","  (model_emotion): ViTForImageClassification(\n","    (vit): ViTModel(\n","      (embeddings): ViTEmbeddings(\n","        (patch_embeddings): ViTPatchEmbeddings(\n","          (projection): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n","        )\n","        (dropout): Dropout(p=0.0, inplace=False)\n","      )\n","      (encoder): ViTEncoder(\n","        (layer): ModuleList(\n","          (0-11): 12 x ViTLayer(\n","            (attention): ViTAttention(\n","              (attention): ViTSelfAttention(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.0, inplace=False)\n","              )\n","              (output): ViTSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.0, inplace=False)\n","              )\n","            )\n","            (intermediate): ViTIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","              (intermediate_act_fn): GELUActivation()\n","            )\n","            (output): ViTOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.0, inplace=False)\n","            )\n","            (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          )\n","        )\n","      )\n","      (layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","    )\n","    (classifier): Linear(in_features=768, out_features=7, bias=True)\n","  )\n",")"]},"execution_count":47,"metadata":{},"output_type":"execute_result"}],"source":["model = WrapModel()\n","model.eval()\n","model.to(device)"]},{"cell_type":"code","execution_count":12,"metadata":{"execution":{"iopub.execute_input":"2024-01-19T02:40:58.674682Z","iopub.status.busy":"2024-01-19T02:40:58.674285Z","iopub.status.idle":"2024-01-19T02:41:01.670092Z","shell.execute_reply":"2024-01-19T02:41:01.669285Z","shell.execute_reply.started":"2024-01-19T02:40:58.674650Z"},"trusted":true},"outputs":[],"source":["import tensorflow as tf\n","gpus = tf.config.experimental.list_physical_devices('GPU')\n","for gpu in gpus:\n","    tf.config.experimental.set_memory_growth(gpu, True)"]},{"cell_type":"code","execution_count":54,"metadata":{"execution":{"iopub.execute_input":"2024-01-19T14:43:34.512464Z","iopub.status.busy":"2024-01-19T14:43:34.511425Z","iopub.status.idle":"2024-01-19T14:43:34.525918Z","shell.execute_reply":"2024-01-19T14:43:34.524897Z","shell.execute_reply.started":"2024-01-19T14:43:34.512425Z"},"trusted":true},"outputs":[],"source":["from deepface import DeepFace\n","from tqdm import tqdm\n","\n","\n","df = pd.DataFrame(columns=[\"file_name\",\n","                           \"bbox\",\n","                           \"image_id\",\n","                           \"race\",\n","                           \"age\",\n","                           \"emotion\", \n","                           \"gender\",\n","                           \"skintone\",\n","                           \"masked\"], index=None)\n","\n","csv_file = '/kaggle/input/fac-public-data-test-1/answer.csv'\n","df_old = pd.read_csv(csv_file)\n","image_dir = '/kaggle/input/fac-public-data-test-1/public_test/public_test'"]},{"cell_type":"code","execution_count":55,"metadata":{"execution":{"iopub.execute_input":"2024-01-19T14:43:36.525973Z","iopub.status.busy":"2024-01-19T14:43:36.524911Z","iopub.status.idle":"2024-01-19T14:43:36.536470Z","shell.execute_reply":"2024-01-19T14:43:36.535299Z","shell.execute_reply.started":"2024-01-19T14:43:36.525936Z"},"trusted":true},"outputs":[],"source":["import json\n","json_file_path = '/kaggle/input/fac-public-data-test-1/file_name_to_image_id.json'\n","with open(json_file_path, 'r') as json_file:\n","    data_name_to_id = json.load(json_file)\n","    \n","all_files = os.listdir(image_dir)"]},{"cell_type":"code","execution_count":56,"metadata":{"execution":{"iopub.execute_input":"2024-01-19T14:43:37.812917Z","iopub.status.busy":"2024-01-19T14:43:37.812158Z","iopub.status.idle":"2024-01-19T14:43:37.819596Z","shell.execute_reply":"2024-01-19T14:43:37.818623Z","shell.execute_reply.started":"2024-01-19T14:43:37.812880Z"},"trusted":true},"outputs":[{"data":{"text/plain":["{'age': ['20-30s', '40-50s', 'Baby', 'Kid', 'Senior', 'Teenager'],\n"," 'race': array(['Caucasian', 'Mongoloid', 'Negroid'], dtype=object),\n"," 'masked': array(['unmasked', 'masked'], dtype=object),\n"," 'skintone': array(['mid-light', 'light', 'mid-dark', 'dark'], dtype=object),\n"," 'emotion': ['Anger',\n","  'Disgust',\n","  'Fear',\n","  'Happiness',\n","  'Neutral',\n","  'Sadness',\n","  'Surprise'],\n"," 'gender': array(['Male', 'Female'], dtype=object)}"]},"execution_count":56,"metadata":{},"output_type":"execute_result"}],"source":["labels_set"]},{"cell_type":"code","execution_count":57,"metadata":{"execution":{"iopub.execute_input":"2024-01-19T14:43:39.495097Z","iopub.status.busy":"2024-01-19T14:43:39.494344Z","iopub.status.idle":"2024-01-19T14:50:16.162532Z","shell.execute_reply":"2024-01-19T14:50:16.161612Z","shell.execute_reply.started":"2024-01-19T14:43:39.495061Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["Processing images: 100%|██████████| 2184/2184 [06:36<00:00,  5.51it/s]\n"]}],"source":["for image_name in tqdm(all_files, desc=\"Processing images\"):\n","    with torch.no_grad():\n","\n","        image_path = os.path.join(image_dir, image_name)\n","        #img = Image.open(image_path)\n","        #if img.mode != 'RGB':\n","        #    img = img.convert('RGB')\n","        #img = np.array(img)\n","\n","        faces = DeepFace.extract_faces(img_path = image_path, \n","            target_size = (224, 224), \n","            detector_backend = backends[6],\n","            enforce_detection = False\n","            )\n","\n","        if isinstance(faces, list):\n","            for i in range(len(faces)):\n","                bboxs = faces[i]['facial_area']\n","                x, y, w, h = bboxs['x'], bboxs['y'], bboxs['w'], bboxs['h']\n","                \n","                \n","                temp = faces[i]['face'].copy()\n","                face = torch.Tensor(temp).permute(2, 0, 1)\n","\n","                results = model(face.unsqueeze(0).to(device))\n","                    \n","                #print(results)\n","                \n","                bbox = str([x, y, w, h])\n","                #print(skintone_race['race'].argmax().item())\n","\n","                id = data_name_to_id[image_name]\n","                idx_skin = results['skintone'].argmax().item()\n","                idx_race = results['race'].argmax().item()\n","                idx_masked = results['masked'].argmax().item()\n","                idx_gender = results['gender'].argmax().item()\n","                idx_emotion = results['emotion'][0].argmax().item()\n","                idx_age = results['age'][0].argmax().item()\n","\n","                #print(idx_emotion, idx_age)\n","                \n","                new_data = {\n","                    'file_name': image_name,\n","                    'bbox': bbox,\n","                    'image_id': id,\n","                    'race': labels_set['race'][idx_race],\n","                    'age': labels_set['age'][idx_age],\n","                    'emotion': labels_set['emotion'][idx_emotion],\n","                    'gender' : labels_set['gender'][idx_gender],\n","                    'skintone': labels_set['skintone'][idx_skin],\n","                    'masked' : labels_set['masked'][idx_masked]\n","                }\n","\n","                df = pd.concat([df, pd.DataFrame([new_data])], ignore_index=True)\n","                del face, temp\n","        else:\n","            id = data_name_to_id[image_name]\n","            new_data = {\n","                    'file_name': image_name,\n","                    'bbox': '[0, 0, 10, 10]',\n","                    'image_id': id,\n","                    'race': 'None',\n","                    'age': 'None',\n","                    'emotion': 'None',\n","                    'gender' : 'None',\n","                    'skintone': 'None',\n","                    'masked' : 'None'\n","            }\n","            df = pd.concat([df, pd.DataFrame([new_data])], ignore_index=True)\n","        torch.cuda.empty_cache()"]},{"cell_type":"code","execution_count":68,"metadata":{"execution":{"iopub.execute_input":"2024-01-19T14:54:33.768192Z","iopub.status.busy":"2024-01-19T14:54:33.767788Z","iopub.status.idle":"2024-01-19T14:54:33.788631Z","shell.execute_reply":"2024-01-19T14:54:33.787693Z","shell.execute_reply.started":"2024-01-19T14:54:33.768160Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["CSV file is saved as answer1.csv.\n"]}],"source":["updated_csv_file = 'answer1.csv'\n","df.to_csv('/kaggle/working/answer.csv', index=False)\n","print(f\"CSV file is saved as {updated_csv_file}.\")"]},{"cell_type":"code","execution_count":61,"metadata":{"execution":{"iopub.execute_input":"2024-01-19T14:52:03.482992Z","iopub.status.busy":"2024-01-19T14:52:03.482191Z","iopub.status.idle":"2024-01-19T14:52:03.499099Z","shell.execute_reply":"2024-01-19T14:52:03.498136Z","shell.execute_reply.started":"2024-01-19T14:52:03.482957Z"},"trusted":true},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>file_name</th>\n","      <th>bbox</th>\n","      <th>image_id</th>\n","      <th>race</th>\n","      <th>age</th>\n","      <th>emotion</th>\n","      <th>gender</th>\n","      <th>skintone</th>\n","      <th>masked</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>69408896.jpg</td>\n","      <td>[601, 351, 206, 333]</td>\n","      <td>991</td>\n","      <td>Mongoloid</td>\n","      <td>20-30s</td>\n","      <td>Happiness</td>\n","      <td>Female</td>\n","      <td>light</td>\n","      <td>unmasked</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>69934968.jpg</td>\n","      <td>[853, 247, 248, 309]</td>\n","      <td>1010</td>\n","      <td>Caucasian</td>\n","      <td>20-30s</td>\n","      <td>Anger</td>\n","      <td>Female</td>\n","      <td>light</td>\n","      <td>masked</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>3410477.jpg</td>\n","      <td>[630, 259, 477, 721]</td>\n","      <td>214</td>\n","      <td>Mongoloid</td>\n","      <td>Senior</td>\n","      <td>Fear</td>\n","      <td>Male</td>\n","      <td>light</td>\n","      <td>unmasked</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>44989956.jpg</td>\n","      <td>[1365, 263, 170, 229]</td>\n","      <td>376</td>\n","      <td>Caucasian</td>\n","      <td>20-30s</td>\n","      <td>Happiness</td>\n","      <td>Female</td>\n","      <td>light</td>\n","      <td>unmasked</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>79053495.jpg</td>\n","      <td>[397, 110, 303, 417]</td>\n","      <td>1316</td>\n","      <td>Mongoloid</td>\n","      <td>20-30s</td>\n","      <td>Neutral</td>\n","      <td>Female</td>\n","      <td>light</td>\n","      <td>masked</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>2253</th>\n","      <td>image_7495.jpg</td>\n","      <td>[181, 254, 605, 686]</td>\n","      <td>2092</td>\n","      <td>Caucasian</td>\n","      <td>Kid</td>\n","      <td>Happiness</td>\n","      <td>Female</td>\n","      <td>mid-light</td>\n","      <td>unmasked</td>\n","    </tr>\n","    <tr>\n","      <th>2254</th>\n","      <td>image_4584.jpg</td>\n","      <td>[234, 211, 583, 703]</td>\n","      <td>1950</td>\n","      <td>Caucasian</td>\n","      <td>20-30s</td>\n","      <td>Neutral</td>\n","      <td>Female</td>\n","      <td>light</td>\n","      <td>unmasked</td>\n","    </tr>\n","    <tr>\n","      <th>2255</th>\n","      <td>12050001.jpg</td>\n","      <td>[493, 206, 338, 479]</td>\n","      <td>61</td>\n","      <td>Caucasian</td>\n","      <td>40-50s</td>\n","      <td>Neutral</td>\n","      <td>Male</td>\n","      <td>light</td>\n","      <td>unmasked</td>\n","    </tr>\n","    <tr>\n","      <th>2256</th>\n","      <td>67251347.jpg</td>\n","      <td>[1064, 208, 272, 387]</td>\n","      <td>910</td>\n","      <td>Mongoloid</td>\n","      <td>20-30s</td>\n","      <td>Happiness</td>\n","      <td>Male</td>\n","      <td>mid-light</td>\n","      <td>unmasked</td>\n","    </tr>\n","    <tr>\n","      <th>2257</th>\n","      <td>image_6902.jpg</td>\n","      <td>[259, 199, 627, 794]</td>\n","      <td>2060</td>\n","      <td>Caucasian</td>\n","      <td>20-30s</td>\n","      <td>Happiness</td>\n","      <td>Female</td>\n","      <td>light</td>\n","      <td>unmasked</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>2258 rows × 9 columns</p>\n","</div>"],"text/plain":["           file_name                   bbox image_id       race     age  \\\n","0       69408896.jpg   [601, 351, 206, 333]      991  Mongoloid  20-30s   \n","1       69934968.jpg   [853, 247, 248, 309]     1010  Caucasian  20-30s   \n","2        3410477.jpg   [630, 259, 477, 721]      214  Mongoloid  Senior   \n","3       44989956.jpg  [1365, 263, 170, 229]      376  Caucasian  20-30s   \n","4       79053495.jpg   [397, 110, 303, 417]     1316  Mongoloid  20-30s   \n","...              ...                    ...      ...        ...     ...   \n","2253  image_7495.jpg   [181, 254, 605, 686]     2092  Caucasian     Kid   \n","2254  image_4584.jpg   [234, 211, 583, 703]     1950  Caucasian  20-30s   \n","2255    12050001.jpg   [493, 206, 338, 479]       61  Caucasian  40-50s   \n","2256    67251347.jpg  [1064, 208, 272, 387]      910  Mongoloid  20-30s   \n","2257  image_6902.jpg   [259, 199, 627, 794]     2060  Caucasian  20-30s   \n","\n","        emotion  gender   skintone    masked  \n","0     Happiness  Female      light  unmasked  \n","1         Anger  Female      light    masked  \n","2          Fear    Male      light  unmasked  \n","3     Happiness  Female      light  unmasked  \n","4       Neutral  Female      light    masked  \n","...         ...     ...        ...       ...  \n","2253  Happiness  Female  mid-light  unmasked  \n","2254    Neutral  Female      light  unmasked  \n","2255    Neutral    Male      light  unmasked  \n","2256  Happiness    Male  mid-light  unmasked  \n","2257  Happiness  Female      light  unmasked  \n","\n","[2258 rows x 9 columns]"]},"execution_count":61,"metadata":{},"output_type":"execute_result"}],"source":["df"]},{"cell_type":"code","execution_count":65,"metadata":{"collapsed":true,"execution":{"iopub.execute_input":"2024-01-19T14:53:10.111463Z","iopub.status.busy":"2024-01-19T14:53:10.111011Z","iopub.status.idle":"2024-01-19T14:53:10.141132Z","shell.execute_reply":"2024-01-19T14:53:10.140138Z","shell.execute_reply.started":"2024-01-19T14:53:10.111428Z"},"jupyter":{"outputs_hidden":true},"trusted":true},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>file_name</th>\n","      <th>bbox</th>\n","      <th>image_id</th>\n","      <th>race</th>\n","      <th>age</th>\n","      <th>emotion</th>\n","      <th>gender</th>\n","      <th>skintone</th>\n","      <th>masked</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>69408896.jpg</td>\n","      <td>[601, 351, 206, 333]</td>\n","      <td>991</td>\n","      <td>Mongoloid</td>\n","      <td>20-30s</td>\n","      <td>Happiness</td>\n","      <td>Female</td>\n","      <td>light</td>\n","      <td>unmasked</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>69934968.jpg</td>\n","      <td>[853, 247, 248, 309]</td>\n","      <td>1010</td>\n","      <td>Caucasian</td>\n","      <td>20-30s</td>\n","      <td>Anger</td>\n","      <td>Female</td>\n","      <td>light</td>\n","      <td>masked</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>3410477.jpg</td>\n","      <td>[630, 259, 477, 721]</td>\n","      <td>214</td>\n","      <td>Mongoloid</td>\n","      <td>Senior</td>\n","      <td>Fear</td>\n","      <td>Male</td>\n","      <td>light</td>\n","      <td>unmasked</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>44989956.jpg</td>\n","      <td>[1365, 263, 170, 229]</td>\n","      <td>376</td>\n","      <td>Caucasian</td>\n","      <td>20-30s</td>\n","      <td>Happiness</td>\n","      <td>Female</td>\n","      <td>light</td>\n","      <td>unmasked</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>79053495.jpg</td>\n","      <td>[397, 110, 303, 417]</td>\n","      <td>1316</td>\n","      <td>Mongoloid</td>\n","      <td>20-30s</td>\n","      <td>Neutral</td>\n","      <td>Female</td>\n","      <td>light</td>\n","      <td>masked</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>2253</th>\n","      <td>image_7495.jpg</td>\n","      <td>[181, 254, 605, 686]</td>\n","      <td>2092</td>\n","      <td>Caucasian</td>\n","      <td>Kid</td>\n","      <td>Happiness</td>\n","      <td>Female</td>\n","      <td>mid-light</td>\n","      <td>unmasked</td>\n","    </tr>\n","    <tr>\n","      <th>2254</th>\n","      <td>image_4584.jpg</td>\n","      <td>[234, 211, 583, 703]</td>\n","      <td>1950</td>\n","      <td>Caucasian</td>\n","      <td>20-30s</td>\n","      <td>Neutral</td>\n","      <td>Female</td>\n","      <td>light</td>\n","      <td>unmasked</td>\n","    </tr>\n","    <tr>\n","      <th>2255</th>\n","      <td>12050001.jpg</td>\n","      <td>[493, 206, 338, 479]</td>\n","      <td>61</td>\n","      <td>Caucasian</td>\n","      <td>40-50s</td>\n","      <td>Neutral</td>\n","      <td>Male</td>\n","      <td>light</td>\n","      <td>unmasked</td>\n","    </tr>\n","    <tr>\n","      <th>2256</th>\n","      <td>67251347.jpg</td>\n","      <td>[1064, 208, 272, 387]</td>\n","      <td>910</td>\n","      <td>Mongoloid</td>\n","      <td>20-30s</td>\n","      <td>Happiness</td>\n","      <td>Male</td>\n","      <td>mid-light</td>\n","      <td>unmasked</td>\n","    </tr>\n","    <tr>\n","      <th>2257</th>\n","      <td>image_6902.jpg</td>\n","      <td>[259, 199, 627, 794]</td>\n","      <td>2060</td>\n","      <td>Caucasian</td>\n","      <td>20-30s</td>\n","      <td>Happiness</td>\n","      <td>Female</td>\n","      <td>light</td>\n","      <td>unmasked</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>2258 rows × 9 columns</p>\n","</div>"],"text/plain":["           file_name                   bbox  image_id       race     age  \\\n","0       69408896.jpg   [601, 351, 206, 333]       991  Mongoloid  20-30s   \n","1       69934968.jpg   [853, 247, 248, 309]      1010  Caucasian  20-30s   \n","2        3410477.jpg   [630, 259, 477, 721]       214  Mongoloid  Senior   \n","3       44989956.jpg  [1365, 263, 170, 229]       376  Caucasian  20-30s   \n","4       79053495.jpg   [397, 110, 303, 417]      1316  Mongoloid  20-30s   \n","...              ...                    ...       ...        ...     ...   \n","2253  image_7495.jpg   [181, 254, 605, 686]      2092  Caucasian     Kid   \n","2254  image_4584.jpg   [234, 211, 583, 703]      1950  Caucasian  20-30s   \n","2255    12050001.jpg   [493, 206, 338, 479]        61  Caucasian  40-50s   \n","2256    67251347.jpg  [1064, 208, 272, 387]       910  Mongoloid  20-30s   \n","2257  image_6902.jpg   [259, 199, 627, 794]      2060  Caucasian  20-30s   \n","\n","        emotion  gender   skintone    masked  \n","0     Happiness  Female      light  unmasked  \n","1         Anger  Female      light    masked  \n","2          Fear    Male      light  unmasked  \n","3     Happiness  Female      light  unmasked  \n","4       Neutral  Female      light    masked  \n","...         ...     ...        ...       ...  \n","2253  Happiness  Female  mid-light  unmasked  \n","2254    Neutral  Female      light  unmasked  \n","2255    Neutral    Male      light  unmasked  \n","2256  Happiness    Male  mid-light  unmasked  \n","2257  Happiness  Female      light  unmasked  \n","\n","[2258 rows x 9 columns]"]},"execution_count":65,"metadata":{},"output_type":"execute_result"}],"source":["pd.read_csv('answer.csv')"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[{"datasetId":4260595,"sourceId":7338576,"sourceType":"datasetVersion"},{"datasetId":4282518,"sourceId":7399404,"sourceType":"datasetVersion"},{"datasetId":4324812,"sourceId":7431792,"sourceType":"datasetVersion"},{"datasetId":4323076,"sourceId":7432662,"sourceType":"datasetVersion"},{"datasetId":4327265,"sourceId":7435394,"sourceType":"datasetVersion"},{"datasetId":4327359,"sourceId":7435529,"sourceType":"datasetVersion"}],"dockerImageVersionId":30636,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"}},"nbformat":4,"nbformat_minor":4}
